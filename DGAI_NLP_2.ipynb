{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT6dZoJf39OC",
        "outputId": "a40600ad-a172-463e-b1b3-1f7a93ca1082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Три девицы под окном\r\n",
            "Пряли поздно вечерком.\r\n",
            "«Кабы я была царица, —\r\n",
            "Говорит одна девица, —\r\n",
            "То на весь крещеный мир\r\n",
            "Приготовила б я пир».\r\n",
            "«Кабы я была царица, —\r\n",
            "Говорит ее сестрица, —\r\n",
            "То на весь бы мир одна\r\n",
            "Наткала я полотна».\r\n"
          ]
        }
      ],
      "source": [
        "!head -10 data.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import math\n",
        "import typing as t\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ZnosHOWbpOfb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_char_vocab(filename: str) -> t.Dict[str, int]:\n",
        "  vocab = {\n",
        "      '<start>': 0,\n",
        "      '<end>': 1,\n",
        "      '<pad>': 2,\n",
        "      '<unk>': 3\n",
        "  }\n",
        "\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "      line = line.replace('»', '')\n",
        "      line = line.replace('«', '')\n",
        "      line = line.replace('—', '')\n",
        "      line = line.replace('…', '')\n",
        "\n",
        "      for char in line:\n",
        "        # char = char.lower()\n",
        "        if char not in vocab:\n",
        "          vocab[char] = len(vocab)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "sZSAxJcxpCv4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_char_vocab('data.txt')"
      ],
      "metadata": {
        "id": "N13iLR36y5rK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(filename: str, sort: bool = True) -> t.Dict[str, int]:\n",
        "  counter: t.Dict[str, int] = {}\n",
        "\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "      line = line.replace('»', '')\n",
        "      line = line.replace('«', '')\n",
        "      line = line.replace('—', '')\n",
        "      line = line.replace('…', '')\n",
        "\n",
        "      for char in line:\n",
        "        # char = char.lower()\n",
        "        counter[char] = counter.get(char, 0) + 1\n",
        "\n",
        "  if sort:\n",
        "    counter = dict(sorted(counter.items(), key=lambda kv: kv[1], reverse=True))\n",
        "\n",
        "  return counter"
      ],
      "metadata": {
        "id": "cR0HAtkutFks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_counter = count_words('data.txt')\n",
        "print(char_counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI0D3UmTuVnx",
        "outputId": "1ad9f221-239c-4878-f0ac-c44f7bdfaf23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 3031, 'о': 1828, 'а': 1462, 'е': 1452, 'т': 1292, 'и': 1072, '\\n': 995, 'р': 869, 'н': 846, 'л': 844, 'с': 825, 'в': 821, 'д': 628, 'у': 556, 'к': 512, 'м': 481, 'ь': 439, 'я': 432, 'п': 397, 'б': 355, 'ы': 305, 'г': 303, 'з': 283, 'й': 274, 'х': 198, 'ч': 173, 'ж': 166, 'ц': 162, 'ш': 148, 'В': 128, 'С': 116, 'И': 104, 'ю': 96, 'К': 74, 'Г': 66, 'П': 62, 'Н': 58, 'А': 57, 'Т': 52, 'Д': 51, 'М': 51, 'О': 49, 'Ч': 46, 'Б': 42, 'щ': 37, 'З': 30, 'Ц': 25, 'Л': 23, 'У': 14, 'ъ': 13, 'Р': 12, 'Е': 12, 'ё': 10, 'Э': 7, 'Я': 6, 'Х': 6, 'э': 6, 'Ш': 5, 'Ж': 2, 'ф': 2, 'Ф': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_pair(token_ids: t.List[int], seq_length: int, pad_token: int) -> t.Tuple[str, str]:\n",
        "  if len(token_ids) > seq_length:\n",
        "    _token_ids = token_ids[:seq_length]\n",
        "  elif len(token_ids) < seq_length:\n",
        "    n_tokens_to_pad = (seq_length - len(token_ids))\n",
        "    _token_ids = token_ids + [pad_token] * n_tokens_to_pad\n",
        "  else:\n",
        "    _token_ids = token_ids\n",
        "\n",
        "  source_tokens = _token_ids[:-1]\n",
        "  target_tokens = _token_ids[1:]\n",
        "  return source_tokens, target_tokens"
      ],
      "metadata": {
        "id": "xz-hEzWSunUU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_train_pair(token_ids=list(range(10)), seq_length=10, pad_token=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_JHGVVNzDeb",
        "outputId": "1de76946-0287-4df2-fb5d-280959d8f236"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_string(vocab: t.Dict[str, int], text: str) -> t.List[int]:\n",
        "  tokens = [vocab.get(char, vocab['<pad>']) for char in text]\n",
        "  if vocab['<pad>'] in tokens:\n",
        "    print('ALERT PADDING!!!')\n",
        "    print(text)\n",
        "    print(tokens)\n",
        "    print('pad index', tokens.index(vocab['<pad>']))\n",
        "    print('pad value', text[tokens.index(vocab['<pad>'])])\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "4w4KCPG5yiNi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_string(vocab, 'кек лол арбидол')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEie9q-y2Xg",
        "outputId": "1b498eb8-ccc4-45d1-962c-9d4643b64ceb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 9, 15, 7, 21, 14, 21, 7, 25, 5, 26, 6, 8, 14, 21]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_train_pair(tokenize_string(vocab, 'кек лол арбидол'), seq_length=20, pad_token=vocab['<pad>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Bd2RSzunW_",
        "outputId": "e824720b-20b3-4fe5-fb9a-1981e6a582ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([15, 9, 15, 7, 21, 14, 21, 7, 25, 5, 26, 6, 8, 14, 21, 2, 2, 2, 2],\n",
              " [9, 15, 7, 21, 14, 21, 7, 25, 5, 26, 6, 8, 14, 21, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(\n",
        "    vocab: t.Dict[str, int],\n",
        "    data: str,\n",
        "    context_length: int,\n",
        "    batch_size: int,\n",
        "    n_batches: int\n",
        ") -> t.List[t.List[int]]:\n",
        "\n",
        "  total_examples = 0\n",
        "\n",
        "  while total_examples < n_batches:\n",
        "\n",
        "    source_batch = []\n",
        "    target_batch = []\n",
        "    sample_indexes = torch.randint(0, len(data) - (context_length + 1), size=(batch_size,))\n",
        "\n",
        "    for index in sample_indexes.tolist():\n",
        "      text = data[index: index + context_length + 1]\n",
        "      source_ids, target_ids = make_train_pair(\n",
        "          tokenize_string(vocab, text),\n",
        "          seq_length=context_length + 1,\n",
        "          pad_token=vocab['<pad>']\n",
        "      )\n",
        "      source_batch.append(source_ids)\n",
        "      target_batch.append(target_ids)\n",
        "\n",
        "    yield torch.LongTensor(source_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "    total_examples += 1"
      ],
      "metadata": {
        "id": "w1dzJp3jzWnK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2char = {pos: char for char, pos in vocab.items()}"
      ],
      "metadata": {
        "id": "VFu2rIad1TBN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab: t.Dict[str, int], n_pos: int, emb_dim: int) -> None:\n",
        "    super().__init__()\n",
        "    self.vocab_embed = nn.Embedding(len(vocab), emb_dim)\n",
        "    self.pos_embed = nn.Embedding(n_pos, emb_dim)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    b, t = x.shape\n",
        "    x_embed = self.vocab_embed(x)\n",
        "    pos_x = self.pos_embed(torch.arange(t)).unsqueeze(0)\n",
        "    return x_embed + pos_x\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, emb_dim: int, n_heads: int, context_length: int) -> None:\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = emb_dim // n_heads\n",
        "    self.q_proj = nn.Linear(self.head_dim, self.head_dim)\n",
        "    self.k_proj = nn.Linear(self.head_dim, self.head_dim)\n",
        "    self.v_proj = nn.Linear(self.head_dim, self.head_dim)\n",
        "    self.output = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch, seq_len, emb = x.shape\n",
        "    x = x.view((batch, seq_len, self.n_heads, self.head_dim)).transpose(1, 2)\n",
        "    # x: [batch, n_heads, seq_len, head_dim]\n",
        "    q = self.q_proj(x)\n",
        "    k = self.k_proj(x)\n",
        "    v = self.v_proj(x)\n",
        "\n",
        "    pos_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "\n",
        "    attention = q @ k.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
        "    attention = attention.masked_fill(pos_mask == 0, -1e9)\n",
        "    attention = nn.Softmax(dim=3)(attention)\n",
        "    attention_values = attention @ v\n",
        "\n",
        "    attention_values = attention_values.transpose(1, 2).contiguous()\n",
        "    attention_values = attention_values.view(batch, seq_len, emb)\n",
        "    x = self.output(attention_values)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, emb_dim: int, n_heads: int, context_length: int) -> None:\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(emb_dim)\n",
        "    self.ln_2 = nn.LayerNorm(emb_dim)\n",
        "    self.mh_at = MultiHeadAttentionLayer(emb_dim, n_heads, context_length)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(emb_dim, 4 * emb_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * emb_dim, emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = x + self.mh_at(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class CustomGPT(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab: t.Dict[str, int],\n",
        "      n_pos: int,\n",
        "      emb_dim: int,\n",
        "      n_layers: int,\n",
        "      n_heads: int,\n",
        "      context_length: int,\n",
        "      drop: float = 0.3\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(vocab, n_pos, emb_dim)\n",
        "    self.drop = nn.Dropout(drop)\n",
        "    self.layers = nn.ModuleList([\n",
        "        TransformerBlock(emb_dim=emb_dim, n_heads=n_heads, context_length=context_length)\n",
        "        for _ in range(n_layers)\n",
        "    ])\n",
        "    self.output_head = nn.Linear(emb_dim, len(vocab))\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.drop(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    x = self.drop(x)\n",
        "    return self.output_head(x)"
      ],
      "metadata": {
        "id": "T-qaAydw6C4b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: t.Iterable,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
        "    criteria: t.Callable\n",
        ") -> float:\n",
        "\n",
        "  epoch_loss = 0.\n",
        "  n_iter = 0\n",
        "\n",
        "  for i, (source_batch, target_batch) in enumerate(loader):\n",
        "    outputs = model(source_batch)\n",
        "    # outputs: [batch, seq_len, n_chars]\n",
        "    outputs = outputs.reshape(outputs.shape[0] * outputs.shape[1], -1)\n",
        "    target_batch = target_batch.ravel()\n",
        "\n",
        "    loss = criteria(outputs, target_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss += loss.detach().item()\n",
        "    n_iter += 1\n",
        "\n",
        "  return epoch_loss"
      ],
      "metadata": {
        "id": "llT_XreFeedn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_line(line: str) -> str:\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "  line = line.replace('»', '')\n",
        "  line = line.replace('«', '')\n",
        "  line = line.replace('—', '')\n",
        "  line = line.replace('…', '')\n",
        "  return line"
      ],
      "metadata": {
        "id": "vlWNLTBmtF8T"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.txt', 'r') as f:\n",
        "  text_data = ''.join([process_line(line) for line in f.readlines()])\n",
        "\n",
        "n_epochs = 200\n",
        "context_length = 64\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "gpt = CustomGPT(vocab, n_pos=context_length, context_length=context_length, emb_dim=128, n_layers=4, n_heads=2, drop=0.3)\n",
        "optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-3, betas=(0.7, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100 * 100, 100 * 125, 100 * 150], gamma=0.1)"
      ],
      "metadata": {
        "id": "xpivHVrV8oX6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.train()\n",
        "for e in range(n_epochs):\n",
        "  loader = create_batches(vocab, data=text_data, batch_size=4, context_length=context_length, n_batches=100)\n",
        "  epoch_loss = train_epoch(gpt, loader, optimizer, scheduler, criteria)\n",
        "  print(f'Epoch: {e + 1}. Loss: {epoch_loss}. LR: {optimizer.param_groups[0][\"lr\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KzefaJFtVrv",
        "outputId": "3fc7174d-e497-4897-f15e-f109552fba85"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Loss: 315.4009072780609. LR: 0.001\n",
            "Epoch: 2. Loss: 278.50137639045715. LR: 0.001\n",
            "Epoch: 3. Loss: 271.28574323654175. LR: 0.001\n",
            "Epoch: 4. Loss: 264.87704062461853. LR: 0.001\n",
            "Epoch: 5. Loss: 261.3731110095978. LR: 0.001\n",
            "Epoch: 6. Loss: 259.10265135765076. LR: 0.001\n",
            "Epoch: 7. Loss: 255.69798135757446. LR: 0.001\n",
            "Epoch: 8. Loss: 253.8404791355133. LR: 0.001\n",
            "Epoch: 9. Loss: 250.82668232917786. LR: 0.001\n",
            "Epoch: 10. Loss: 249.66340327262878. LR: 0.001\n",
            "Epoch: 11. Loss: 245.6688015460968. LR: 0.001\n",
            "Epoch: 12. Loss: 244.00245881080627. LR: 0.001\n",
            "Epoch: 13. Loss: 238.95386719703674. LR: 0.001\n",
            "Epoch: 14. Loss: 235.5850486755371. LR: 0.001\n",
            "Epoch: 15. Loss: 234.049156665802. LR: 0.001\n",
            "Epoch: 16. Loss: 229.340638756752. LR: 0.001\n",
            "Epoch: 17. Loss: 223.88769233226776. LR: 0.001\n",
            "Epoch: 18. Loss: 219.81612622737885. LR: 0.001\n",
            "Epoch: 19. Loss: 219.05036544799805. LR: 0.001\n",
            "Epoch: 20. Loss: 212.68632125854492. LR: 0.001\n",
            "Epoch: 21. Loss: 211.07406437397003. LR: 0.001\n",
            "Epoch: 22. Loss: 202.68110620975494. LR: 0.001\n",
            "Epoch: 23. Loss: 199.09206700325012. LR: 0.001\n",
            "Epoch: 24. Loss: 197.32085490226746. LR: 0.001\n",
            "Epoch: 25. Loss: 195.41189908981323. LR: 0.001\n",
            "Epoch: 26. Loss: 188.3782900571823. LR: 0.001\n",
            "Epoch: 27. Loss: 183.16840279102325. LR: 0.001\n",
            "Epoch: 28. Loss: 178.84154164791107. LR: 0.001\n",
            "Epoch: 29. Loss: 171.33896398544312. LR: 0.001\n",
            "Epoch: 30. Loss: 167.16415441036224. LR: 0.001\n",
            "Epoch: 31. Loss: 163.3153635263443. LR: 0.001\n",
            "Epoch: 32. Loss: 159.42045319080353. LR: 0.001\n",
            "Epoch: 33. Loss: 156.18408489227295. LR: 0.001\n",
            "Epoch: 34. Loss: 151.1187560558319. LR: 0.001\n",
            "Epoch: 35. Loss: 144.541863322258. LR: 0.001\n",
            "Epoch: 36. Loss: 143.15726506710052. LR: 0.001\n",
            "Epoch: 37. Loss: 134.3248432278633. LR: 0.001\n",
            "Epoch: 38. Loss: 136.3516327738762. LR: 0.001\n",
            "Epoch: 39. Loss: 126.79997378587723. LR: 0.001\n",
            "Epoch: 40. Loss: 124.24954390525818. LR: 0.001\n",
            "Epoch: 41. Loss: 122.46645498275757. LR: 0.001\n",
            "Epoch: 42. Loss: 117.59439241886139. LR: 0.001\n",
            "Epoch: 43. Loss: 111.18005585670471. LR: 0.001\n",
            "Epoch: 44. Loss: 110.6847562789917. LR: 0.001\n",
            "Epoch: 45. Loss: 105.39313608407974. LR: 0.001\n",
            "Epoch: 46. Loss: 102.39113438129425. LR: 0.001\n",
            "Epoch: 47. Loss: 101.8933658003807. LR: 0.001\n",
            "Epoch: 48. Loss: 94.65998536348343. LR: 0.001\n",
            "Epoch: 49. Loss: 100.64977821707726. LR: 0.001\n",
            "Epoch: 50. Loss: 90.46926674246788. LR: 0.001\n",
            "Epoch: 51. Loss: 91.08565554022789. LR: 0.001\n",
            "Epoch: 52. Loss: 89.92331874370575. LR: 0.001\n",
            "Epoch: 53. Loss: 85.76798313856125. LR: 0.001\n",
            "Epoch: 54. Loss: 82.09587422013283. LR: 0.001\n",
            "Epoch: 55. Loss: 78.86749550700188. LR: 0.001\n",
            "Epoch: 56. Loss: 78.55846980214119. LR: 0.001\n",
            "Epoch: 57. Loss: 80.63976034522057. LR: 0.001\n",
            "Epoch: 58. Loss: 73.60368347167969. LR: 0.001\n",
            "Epoch: 59. Loss: 73.56174075603485. LR: 0.001\n",
            "Epoch: 60. Loss: 73.61737143993378. LR: 0.001\n",
            "Epoch: 61. Loss: 69.38603854179382. LR: 0.001\n",
            "Epoch: 62. Loss: 68.51728323101997. LR: 0.001\n",
            "Epoch: 63. Loss: 67.0787880718708. LR: 0.001\n",
            "Epoch: 64. Loss: 66.29767253994942. LR: 0.001\n",
            "Epoch: 65. Loss: 62.25228750705719. LR: 0.001\n",
            "Epoch: 66. Loss: 63.523466259241104. LR: 0.001\n",
            "Epoch: 67. Loss: 60.51186728477478. LR: 0.001\n",
            "Epoch: 68. Loss: 58.57776312530041. LR: 0.001\n",
            "Epoch: 69. Loss: 56.099895387887955. LR: 0.001\n",
            "Epoch: 70. Loss: 58.26074409484863. LR: 0.001\n",
            "Epoch: 71. Loss: 56.190719455480576. LR: 0.001\n",
            "Epoch: 72. Loss: 54.67613846063614. LR: 0.001\n",
            "Epoch: 73. Loss: 53.4643149971962. LR: 0.001\n",
            "Epoch: 74. Loss: 53.16598916053772. LR: 0.001\n",
            "Epoch: 75. Loss: 51.36836290359497. LR: 0.001\n",
            "Epoch: 76. Loss: 50.83105055987835. LR: 0.001\n",
            "Epoch: 77. Loss: 51.157393515110016. LR: 0.001\n",
            "Epoch: 78. Loss: 49.72926327586174. LR: 0.001\n",
            "Epoch: 79. Loss: 49.125693678855896. LR: 0.001\n",
            "Epoch: 80. Loss: 48.896361351013184. LR: 0.001\n",
            "Epoch: 81. Loss: 47.49192371964455. LR: 0.001\n",
            "Epoch: 82. Loss: 47.471668273210526. LR: 0.001\n",
            "Epoch: 83. Loss: 45.9793304502964. LR: 0.001\n",
            "Epoch: 84. Loss: 45.72395530343056. LR: 0.001\n",
            "Epoch: 85. Loss: 43.72897171974182. LR: 0.001\n",
            "Epoch: 86. Loss: 46.58326044678688. LR: 0.001\n",
            "Epoch: 87. Loss: 44.9311999976635. LR: 0.001\n",
            "Epoch: 88. Loss: 46.57647719979286. LR: 0.001\n",
            "Epoch: 89. Loss: 42.0466285943985. LR: 0.001\n",
            "Epoch: 90. Loss: 42.73626273870468. LR: 0.001\n",
            "Epoch: 91. Loss: 43.59878695011139. LR: 0.001\n",
            "Epoch: 92. Loss: 40.20778772234917. LR: 0.001\n",
            "Epoch: 93. Loss: 41.74710941314697. LR: 0.001\n",
            "Epoch: 94. Loss: 42.68455424904823. LR: 0.001\n",
            "Epoch: 95. Loss: 39.17314451932907. LR: 0.001\n",
            "Epoch: 96. Loss: 40.05138583481312. LR: 0.001\n",
            "Epoch: 97. Loss: 40.486950039863586. LR: 0.001\n",
            "Epoch: 98. Loss: 38.75860957801342. LR: 0.001\n",
            "Epoch: 99. Loss: 41.099848195910454. LR: 0.001\n",
            "Epoch: 100. Loss: 40.55603328347206. LR: 0.0001\n",
            "Epoch: 101. Loss: 33.3263855278492. LR: 0.0001\n",
            "Epoch: 102. Loss: 29.00061720609665. LR: 0.0001\n",
            "Epoch: 103. Loss: 27.594653949141502. LR: 0.0001\n",
            "Epoch: 104. Loss: 26.30562588572502. LR: 0.0001\n",
            "Epoch: 105. Loss: 26.037835523486137. LR: 0.0001\n",
            "Epoch: 106. Loss: 25.899770721793175. LR: 0.0001\n",
            "Epoch: 107. Loss: 24.50409995019436. LR: 0.0001\n",
            "Epoch: 108. Loss: 25.485871344804764. LR: 0.0001\n",
            "Epoch: 109. Loss: 24.20029105246067. LR: 0.0001\n",
            "Epoch: 110. Loss: 24.342096462845802. LR: 0.0001\n",
            "Epoch: 111. Loss: 23.779856637120247. LR: 0.0001\n",
            "Epoch: 112. Loss: 23.632944583892822. LR: 0.0001\n",
            "Epoch: 113. Loss: 23.754411697387695. LR: 0.0001\n",
            "Epoch: 114. Loss: 22.805841475725174. LR: 0.0001\n",
            "Epoch: 115. Loss: 23.76179200410843. LR: 0.0001\n",
            "Epoch: 116. Loss: 23.551263108849525. LR: 0.0001\n",
            "Epoch: 117. Loss: 23.294049218297005. LR: 0.0001\n",
            "Epoch: 118. Loss: 22.233499586582184. LR: 0.0001\n",
            "Epoch: 119. Loss: 22.815961748361588. LR: 0.0001\n",
            "Epoch: 120. Loss: 23.212156414985657. LR: 0.0001\n",
            "Epoch: 121. Loss: 22.9371814802289. LR: 0.0001\n",
            "Epoch: 122. Loss: 23.537302806973457. LR: 0.0001\n",
            "Epoch: 123. Loss: 22.62800605595112. LR: 0.0001\n",
            "Epoch: 124. Loss: 21.88324584066868. LR: 0.0001\n",
            "Epoch: 125. Loss: 21.748583123087883. LR: 0.0001\n",
            "Epoch: 126. Loss: 21.503449469804764. LR: 0.0001\n",
            "Epoch: 127. Loss: 22.36295196413994. LR: 0.0001\n",
            "Epoch: 128. Loss: 22.280843377113342. LR: 0.0001\n",
            "Epoch: 129. Loss: 21.629442870616913. LR: 0.0001\n",
            "Epoch: 130. Loss: 22.293036073446274. LR: 0.0001\n",
            "Epoch: 131. Loss: 21.940674126148224. LR: 0.0001\n",
            "Epoch: 132. Loss: 21.46756538748741. LR: 0.0001\n",
            "Epoch: 133. Loss: 21.501713931560516. LR: 0.0001\n",
            "Epoch: 134. Loss: 20.951007783412933. LR: 0.0001\n",
            "Epoch: 135. Loss: 21.221327006816864. LR: 0.0001\n",
            "Epoch: 136. Loss: 21.34121821820736. LR: 0.0001\n",
            "Epoch: 137. Loss: 21.0777178555727. LR: 0.0001\n",
            "Epoch: 138. Loss: 21.25581184029579. LR: 0.0001\n",
            "Epoch: 139. Loss: 20.958016946911812. LR: 0.0001\n",
            "Epoch: 140. Loss: 21.31412771344185. LR: 0.0001\n",
            "Epoch: 141. Loss: 20.37724319845438. LR: 0.0001\n",
            "Epoch: 142. Loss: 20.729134902358055. LR: 0.0001\n",
            "Epoch: 143. Loss: 20.98056200146675. LR: 0.0001\n",
            "Epoch: 144. Loss: 21.038516744971275. LR: 0.0001\n",
            "Epoch: 145. Loss: 21.269928857684135. LR: 0.0001\n",
            "Epoch: 146. Loss: 21.0446667522192. LR: 0.0001\n",
            "Epoch: 147. Loss: 21.26889692991972. LR: 0.0001\n",
            "Epoch: 148. Loss: 20.319056563079357. LR: 0.0001\n",
            "Epoch: 149. Loss: 20.491605058312416. LR: 0.0001\n",
            "Epoch: 150. Loss: 20.42012121528387. LR: 1e-05\n",
            "Epoch: 151. Loss: 20.493823193013668. LR: 1e-05\n",
            "Epoch: 152. Loss: 20.324402034282684. LR: 1e-05\n",
            "Epoch: 153. Loss: 20.432719185948372. LR: 1e-05\n",
            "Epoch: 154. Loss: 20.02598687261343. LR: 1e-05\n",
            "Epoch: 155. Loss: 20.879828929901123. LR: 1e-05\n",
            "Epoch: 156. Loss: 20.29242606461048. LR: 1e-05\n",
            "Epoch: 157. Loss: 20.56021013855934. LR: 1e-05\n",
            "Epoch: 158. Loss: 20.369333028793335. LR: 1e-05\n",
            "Epoch: 159. Loss: 19.194344229996204. LR: 1e-05\n",
            "Epoch: 160. Loss: 20.16847587376833. LR: 1e-05\n",
            "Epoch: 161. Loss: 20.077013194561005. LR: 1e-05\n",
            "Epoch: 162. Loss: 19.614409774541855. LR: 1e-05\n",
            "Epoch: 163. Loss: 19.809156090021133. LR: 1e-05\n",
            "Epoch: 164. Loss: 19.822377666831017. LR: 1e-05\n",
            "Epoch: 165. Loss: 19.816123813390732. LR: 1e-05\n",
            "Epoch: 166. Loss: 19.51344495266676. LR: 1e-05\n",
            "Epoch: 167. Loss: 19.753180146217346. LR: 1e-05\n",
            "Epoch: 168. Loss: 18.996518656611443. LR: 1e-05\n",
            "Epoch: 169. Loss: 19.49997939169407. LR: 1e-05\n",
            "Epoch: 170. Loss: 20.011052772402763. LR: 1e-05\n",
            "Epoch: 171. Loss: 20.13935151696205. LR: 1e-05\n",
            "Epoch: 172. Loss: 19.31072424352169. LR: 1e-05\n",
            "Epoch: 173. Loss: 19.56337969005108. LR: 1e-05\n",
            "Epoch: 174. Loss: 19.698763698339462. LR: 1e-05\n",
            "Epoch: 175. Loss: 19.881889030337334. LR: 1e-05\n",
            "Epoch: 176. Loss: 19.92925064265728. LR: 1e-05\n",
            "Epoch: 177. Loss: 19.024566054344177. LR: 1e-05\n",
            "Epoch: 178. Loss: 19.532957293093204. LR: 1e-05\n",
            "Epoch: 179. Loss: 19.454073950648308. LR: 1e-05\n",
            "Epoch: 180. Loss: 19.557093113660812. LR: 1e-05\n",
            "Epoch: 181. Loss: 19.732009686529636. LR: 1e-05\n",
            "Epoch: 182. Loss: 19.551910370588303. LR: 1e-05\n",
            "Epoch: 183. Loss: 18.900990657508373. LR: 1e-05\n",
            "Epoch: 184. Loss: 19.1745575517416. LR: 1e-05\n",
            "Epoch: 185. Loss: 19.72111651301384. LR: 1e-05\n",
            "Epoch: 186. Loss: 19.34915191680193. LR: 1e-05\n",
            "Epoch: 187. Loss: 19.081386998295784. LR: 1e-05\n",
            "Epoch: 188. Loss: 19.056938089430332. LR: 1e-05\n",
            "Epoch: 189. Loss: 19.486336663365364. LR: 1e-05\n",
            "Epoch: 190. Loss: 19.17012956738472. LR: 1e-05\n",
            "Epoch: 191. Loss: 19.225621968507767. LR: 1e-05\n",
            "Epoch: 192. Loss: 19.792238265275955. LR: 1e-05\n",
            "Epoch: 193. Loss: 20.117930471897125. LR: 1e-05\n",
            "Epoch: 194. Loss: 19.035494796931744. LR: 1e-05\n",
            "Epoch: 195. Loss: 19.694026596844196. LR: 1e-05\n",
            "Epoch: 196. Loss: 19.85652457177639. LR: 1e-05\n",
            "Epoch: 197. Loss: 19.241921439766884. LR: 1e-05\n",
            "Epoch: 198. Loss: 19.106278605759144. LR: 1e-05\n",
            "Epoch: 199. Loss: 19.674471333622932. LR: 1e-05\n",
            "Epoch: 200. Loss: 19.120668932795525. LR: 1.0000000000000002e-06\n",
            "Epoch: 201. Loss: 19.527189396321774. LR: 1.0000000000000002e-06\n",
            "Epoch: 202. Loss: 19.285095386207104. LR: 1.0000000000000002e-06\n",
            "Epoch: 203. Loss: 19.064527109265327. LR: 1.0000000000000002e-06\n",
            "Epoch: 204. Loss: 20.05052450299263. LR: 1.0000000000000002e-06\n",
            "Epoch: 205. Loss: 19.730178207159042. LR: 1.0000000000000002e-06\n",
            "Epoch: 206. Loss: 19.57181414961815. LR: 1.0000000000000002e-06\n",
            "Epoch: 207. Loss: 19.353993222117424. LR: 1.0000000000000002e-06\n",
            "Epoch: 208. Loss: 19.430571287870407. LR: 1.0000000000000002e-06\n",
            "Epoch: 209. Loss: 19.47559605538845. LR: 1.0000000000000002e-06\n",
            "Epoch: 210. Loss: 18.778972536325455. LR: 1.0000000000000002e-06\n",
            "Epoch: 211. Loss: 18.860723242163658. LR: 1.0000000000000002e-06\n",
            "Epoch: 212. Loss: 19.583934970200062. LR: 1.0000000000000002e-06\n",
            "Epoch: 213. Loss: 19.05791512131691. LR: 1.0000000000000002e-06\n",
            "Epoch: 214. Loss: 19.308016628026962. LR: 1.0000000000000002e-06\n",
            "Epoch: 215. Loss: 19.425244092941284. LR: 1.0000000000000002e-06\n",
            "Epoch: 216. Loss: 19.712073162198067. LR: 1.0000000000000002e-06\n",
            "Epoch: 217. Loss: 18.920723885297775. LR: 1.0000000000000002e-06\n",
            "Epoch: 218. Loss: 18.91383546590805. LR: 1.0000000000000002e-06\n",
            "Epoch: 219. Loss: 19.836788550019264. LR: 1.0000000000000002e-06\n",
            "Epoch: 220. Loss: 19.367482870817184. LR: 1.0000000000000002e-06\n",
            "Epoch: 221. Loss: 18.650734588503838. LR: 1.0000000000000002e-06\n",
            "Epoch: 222. Loss: 18.651655226945877. LR: 1.0000000000000002e-06\n",
            "Epoch: 223. Loss: 19.51797016710043. LR: 1.0000000000000002e-06\n",
            "Epoch: 224. Loss: 19.495930463075638. LR: 1.0000000000000002e-06\n",
            "Epoch: 225. Loss: 19.312384381890297. LR: 1.0000000000000002e-06\n",
            "Epoch: 226. Loss: 19.452368147671223. LR: 1.0000000000000002e-06\n",
            "Epoch: 227. Loss: 19.516608580946922. LR: 1.0000000000000002e-06\n",
            "Epoch: 228. Loss: 18.86943231523037. LR: 1.0000000000000002e-06\n",
            "Epoch: 229. Loss: 20.004941403865814. LR: 1.0000000000000002e-06\n",
            "Epoch: 230. Loss: 19.40649376809597. LR: 1.0000000000000002e-06\n",
            "Epoch: 231. Loss: 19.574927642941475. LR: 1.0000000000000002e-06\n",
            "Epoch: 232. Loss: 18.9460611641407. LR: 1.0000000000000002e-06\n",
            "Epoch: 233. Loss: 19.100878834724426. LR: 1.0000000000000002e-06\n",
            "Epoch: 234. Loss: 19.043881803750992. LR: 1.0000000000000002e-06\n",
            "Epoch: 235. Loss: 18.795670107007027. LR: 1.0000000000000002e-06\n",
            "Epoch: 236. Loss: 19.159693226218224. LR: 1.0000000000000002e-06\n",
            "Epoch: 237. Loss: 18.397917985916138. LR: 1.0000000000000002e-06\n",
            "Epoch: 238. Loss: 19.10424894094467. LR: 1.0000000000000002e-06\n",
            "Epoch: 239. Loss: 19.587402820587158. LR: 1.0000000000000002e-06\n",
            "Epoch: 240. Loss: 19.819186344742775. LR: 1.0000000000000002e-06\n",
            "Epoch: 241. Loss: 19.909543558955193. LR: 1.0000000000000002e-06\n",
            "Epoch: 242. Loss: 19.124231971800327. LR: 1.0000000000000002e-06\n",
            "Epoch: 243. Loss: 19.98053203523159. LR: 1.0000000000000002e-06\n",
            "Epoch: 244. Loss: 19.001249969005585. LR: 1.0000000000000002e-06\n",
            "Epoch: 245. Loss: 18.605310939252377. LR: 1.0000000000000002e-06\n",
            "Epoch: 246. Loss: 19.630715370178223. LR: 1.0000000000000002e-06\n",
            "Epoch: 247. Loss: 18.840257242321968. LR: 1.0000000000000002e-06\n",
            "Epoch: 248. Loss: 18.523233711719513. LR: 1.0000000000000002e-06\n",
            "Epoch: 249. Loss: 20.07234261929989. LR: 1.0000000000000002e-06\n",
            "Epoch: 250. Loss: 19.256615668535233. LR: 1.0000000000000002e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "\n",
        "@torch.inference_mode\n",
        "def generate(\n",
        "    vocab: t.Dict[str, int],\n",
        "    model: nn.Module,\n",
        "    prompt: str,\n",
        "    temperature: float = 1.0,\n",
        "    max_tokens: int = 20,\n",
        "    top_k: int = 5\n",
        ") -> str:\n",
        "  model.eval()\n",
        "  prompt_ids = tokenize_string(vocab, prompt)\n",
        "  prompt_ids_pt = torch.LongTensor([prompt_ids])\n",
        "  id2char = {pos: char for char, pos in vocab.items()}\n",
        "  result = prompt\n",
        "\n",
        "  for i in range(max_tokens):\n",
        "    output = model(prompt_ids_pt)\n",
        "    topk_v, topk_idx = torch.topk(output[:, -1, :], k=top_k, dim=1)\n",
        "\n",
        "    if temperature > 0.:\n",
        "      topk_v = topk_v / temperature\n",
        "      probs = nn.Softmax(dim=1)(topk_v)\n",
        "\n",
        "      next_index = torch.multinomial(probs, num_samples=1)[0].item()\n",
        "      next_char = topk_idx[0, next_index].item()\n",
        "    else:\n",
        "      next_char = torch.argmax(output[:, -1, :], dim=1).item()\n",
        "\n",
        "    result = result + id2char[next_char]\n",
        "\n",
        "    if prompt_ids_pt.shape[-1] < model.encoder.pos_embed.weight.shape[0]:\n",
        "      prompt_ids_pt = torch.cat([prompt_ids_pt, torch.LongTensor([[next_char]])], dim=1)\n",
        "    else:\n",
        "      prompt_ids_pt = torch.cat([prompt_ids_pt[:, 1:], torch.LongTensor([[next_char]])], dim=1)\n",
        "\n",
        "    if next_char == vocab['<end>']:\n",
        "      print('break')\n",
        "      break\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "t1 = perf_counter()\n",
        "\n",
        "prompt=\"\"\"Три богатыря\"\"\"\n",
        "\n",
        "for _ in range(1):\n",
        "  output_text = generate(vocab, gpt, prompt, temperature=0.5, max_tokens=500)\n",
        "  print(output_text)\n",
        "  print('*' * 89)\n",
        "\n",
        "t2 = perf_counter()\n",
        "\n",
        "print('\\ntime took', t2 - t1, 'sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz9-d0IByTgX",
        "outputId": "69fee532-d466-4694-8c27-80eddc324181"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Три богатыря\n",
            "В чешуе как жар горя\n",
            "Идут витязи четами\n",
            "И блистая сединами\n",
            "Дядька впереди идет\n",
            "И ко граду их ведет\n",
            "С башни князь Гвидон сбегает\n",
            "Дорогих гостей встречает\n",
            "Второпях народ бежит\n",
            "Дядька князю говорит\n",
            "Лебедь нас к тебе послала\n",
            "И наказом наказала\n",
            "Славный город твой хранить\n",
            "И дозором обходить\n",
            "Мы отныне ежеденно\n",
            "Вместе будем непременно\n",
            "У высоких стен твоих\n",
            "Выходить из вод морских\n",
            "Так увидимся мы вскоре\n",
            "А теперь пора нам в море\n",
            "Полетел и зажужжал\n",
            "Судно на море догнал\n",
            "Потихоньку опустился\n",
            "На корабль  и в \n",
            "*****************************************************************************************\n",
            "\n",
            "time took 2.799563878999834 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AoeKQiEhYQNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}