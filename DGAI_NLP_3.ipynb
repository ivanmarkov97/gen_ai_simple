{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYzRnzTx4_7k",
        "outputId": "36755dce-0f94-40ae-97fc-7dc954bd0efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet transformers datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpZ_eAYs_we_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing as t\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import GPT2Config, GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSPMYqz4KtwG"
      },
      "outputs": [],
      "source": [
        "!rm -rf my_tokenizer\n",
        "!rm -rf saltan_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPONPPtiCikn"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      file_path: str,\n",
        "      tokenizer: GPT2TokenizerFast,\n",
        "      context_length: int,\n",
        "  ) -> None:\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "      all_text = f.read()\n",
        "\n",
        "    self.file_path = file_path\n",
        "    self.tokenizer = tokenizer\n",
        "    self.context_length = context_length\n",
        "    self.n_samples = len(all_text) - context_length\n",
        "    self.all_text = all_text\n",
        "\n",
        "  def __getitem__(self, index: int) -> t.Dict[str, torch.Tensor]:\n",
        "\n",
        "    text_data = self.all_text[index: index + self.context_length]\n",
        "\n",
        "    encoded = self.tokenizer(\n",
        "        text_data,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.context_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvbDad3kIjlX"
      },
      "outputs": [],
      "source": [
        "vocab_size = 500\n",
        "context_length = 128\n",
        "\n",
        "gpt2_config = GPT2Config(\n",
        "    vocab_size=vocab_size + 1,\n",
        "    n_positions=context_length,\n",
        "    n_embd=256,\n",
        "    n_layer=4,\n",
        "    n_head=2\n",
        ")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(\n",
        "    files=['data.txt'],\n",
        "    vocab_size=vocab_size,\n",
        "    min_frequency=2,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "if not os.path.exists('my_tokenizer'):\n",
        "  os.mkdir('my_tokenizer')\n",
        "\n",
        "tokenizer.save_model('my_tokenizer')\n",
        "\n",
        "gpt_model = GPT2LMHeadModel(gpt2_config)\n",
        "gpt_tokenizer = GPT2TokenizerFast.from_pretrained('./my_tokenizer')\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "dataset = MyDataset(\n",
        "    tokenizer=gpt_tokenizer,\n",
        "    file_path=\"./data.txt\",\n",
        "    context_length=context_length,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=gpt_tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "uF9f4-wlMOPC",
        "outputId": "b1712c48-6e04-4416-c5e0-00be929dd9d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25307' max='29360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25307/29360 3:30:02 < 33:38, 2.01 it/s, Epoch 8.62/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.369300</td>\n",
              "      <td>0.238741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.240600</td>\n",
              "      <td>0.167502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.144610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.172700</td>\n",
              "      <td>0.135228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.157800</td>\n",
              "      <td>0.119571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.143900</td>\n",
              "      <td>0.115700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.133400</td>\n",
              "      <td>0.107934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.126900</td>\n",
              "      <td>0.099859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./saltan_model\",\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=10,\n",
        "    prediction_loss_only=True,\n",
        "    overwrite_output_dir=True,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVh0CQ8_RPzp"
      },
      "outputs": [],
      "source": [
        "gpt_model = gpt_model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXmurNdROIl2",
        "outputId": "506a1931-7f5e-4cc4-92fd-caedbc50a176"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Белка там живет ручная,\n",
            "Да затейница какая!\n",
            "Белка песенки поет,\n",
            "Да орешки всё грызет,\n",
            "А орешки не простые,\n",
            "Всё скорлупки золоты-тос удь»\n",
            "Ченя у удиве удь удь удь \n"
          ]
        }
      ],
      "source": [
        "model_inputs = gpt_tokenizer('Белка', return_tensors='pt')\n",
        "greedy_output = gpt_model.generate(**model_inputs, max_new_tokens=100)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(gpt_tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpi1xXPwA4qO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}